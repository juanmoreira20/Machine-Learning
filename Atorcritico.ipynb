{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanmoreira20/Machine-Learning/blob/main/Atorcritico.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GE6h7xoCPCC",
        "outputId": "0e3a23c8-44c8-4be6-d38a-611be5e2b607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.7/dist-packages (0.26.3)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (4.13.0)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (0.0.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.10.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gymnasium[classic_control] in /usr/local/lib/python3.7/dist-packages (0.26.3)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from gymnasium[classic_control]) (0.0.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium[classic_control]) (4.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gymnasium[classic_control]) (1.21.6)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 55 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium[classic_control]) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gymnasium[classic_control]) (3.10.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "#instalando bibliotecas que não estão no ambiente do colab\n",
        "!pip install gymnasium\n",
        "!pip install pyglet\n",
        "!pip install gymnasium[classic_control]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports necessários\n",
        "import collections \n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import statistics\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers\n",
        "from typing import Any, List, Sequence, Tuple\n",
        "import tqdm\n",
        "from IPython import  display as ipythondisplay\n",
        "from PIL import Image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "N18duDzUCY-V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setando ambiente através da biblioteca gym \n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# seed\n",
        "seed= 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "#epsilion muito próximo a zero\n",
        "esp=np.finfo(np.float32).eps.item()"
      ],
      "metadata": {
        "id": "mPSJuGbH3BPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_hdh4rkuCPCG"
      },
      "outputs": [],
      "source": [
        "#criação do modelo atorcritico\n",
        "class AtorCritico(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_actions:int,\n",
        "        num_hidden_units:int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.common = layers.Dense(num_hidden_units, activation='relu')\n",
        "        self.ator = layers.Dense(num_actions)\n",
        "        self.critico =layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs:tf.Tensor) -> Tuple[tf.Tensor,tf.Tensor]:\n",
        "        x= self.common(inputs)\n",
        "        return self.ator(x), self.critico(x)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "icKqBUS7CPCH"
      },
      "outputs": [],
      "source": [
        "#definição das camadas do modelo\n",
        "num_actions = env.action_space.n\n",
        "num_hidden_units=128\n",
        "\n",
        "model=AtorCritico(num_actions,num_hidden_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "XEd6BGf_CPCH"
      },
      "outputs": [],
      "source": [
        "#setando os passos no ambiente do pendulo invertido\n",
        "def env_steps(action: np.ndarray)-> Tuple[np.ndarray,np.ndarray,np.ndarray]:\n",
        "    estado, recompenca, final, truncado, info = env.step(action)\n",
        "    return (estado.astype(np.float32), np.array(recompenca, np.int32), np.array(final, np.int32))\n",
        "#transformando função python em função tensor\n",
        "def tf_env_steps(action: tf.Tensor)-> List[tf.Tensor]:\n",
        "    return tf.numpy_function(env_steps, [action],[tf.float32, tf.int32, tf.int32])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hbQWZAyuCPCI"
      },
      "outputs": [],
      "source": [
        "#definindo um episódio baseado no modelo do ator critico\n",
        "def rodar_ep(initial_state:tf.Tensor,\n",
        "    model: tf.keras.Model,\n",
        "    max_steps:int)-> Tuple[tf.Tensor,tf.Tensor,tf.Tensor ]:\n",
        "\n",
        "    acao_probs= tf.TensorArray(dtype=tf.float32,size=0, dynamic_size=True)\n",
        "    valores= tf.TensorArray(dtype=tf.float32,size=0, dynamic_size=True)\n",
        "    recompencas= tf.TensorArray(dtype=tf.int32,size=0, dynamic_size=True)\n",
        "\n",
        "    initial_state_shape=initial_state.shape\n",
        "    estado=initial_state\n",
        "\n",
        "    for t in tf.range(max_steps):\n",
        "        estado=tf.expand_dims(estado, 0)\n",
        "\n",
        "        action_logists_t, value =model(estado)\n",
        "\n",
        "        acao= tf.random.categorical(action_logists_t, 1 )[0,0]\n",
        "        acao_probs_t= tf.nn.softmax(action_logists_t)\n",
        "\n",
        "        valores =valores.write(t, tf.squeeze(value))\n",
        "\n",
        "        acao_probs= acao_probs.write(t, acao_probs_t[0, acao])\n",
        "\n",
        "        estado, recompenca, final =tf_env_steps(acao)\n",
        "        estado.set_shape(initial_state_shape)\n",
        "\n",
        "        recompencas=recompencas.write(t,recompenca)\n",
        "\n",
        "        if tf.cast(final, tf.bool):\n",
        "            break\n",
        "    acoes_prob=acao_probs.stack()\n",
        "    valores= valores.stack()\n",
        "    recompencas= recompencas.stack()\n",
        "\n",
        "    return acoes_prob,valores,recompencas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "i632GoBYCPCJ"
      },
      "outputs": [],
      "source": [
        "#valor predito\n",
        "def receber_valor_esperado(\n",
        "    recompencas: tf.Tensor,\n",
        "    gamma: float,\n",
        "    standardize: bool=True\n",
        ")-> tf.Tensor:\n",
        "    n=tf.shape(recompencas)[0]\n",
        "    returns= tf.TensorArray(dtype=tf.float32, size=n)\n",
        "\n",
        "    recompencas= tf.cast(recompencas[::-1], dtype=tf.float32)\n",
        "    soma_descontada= tf.constant(0.0)\n",
        "    soma_descontada_shape=soma_descontada.shape\n",
        "    for i in tf.range(n):\n",
        "        recompenca=recompencas[i]\n",
        "        soma_descontada= recompenca+gamma*soma_descontada\n",
        "        soma_descontada.set_shape(soma_descontada_shape)\n",
        "        returns= returns.write(i, soma_descontada)\n",
        "    returns= returns.stack()[::-1]\n",
        "    if standardize:\n",
        "        returns= ((returns-tf.math.reduce_mean(returns))/\n",
        "        (tf.math.reduce_std(returns)+esp))\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5ubb-7ywCPCK"
      },
      "outputs": [],
      "source": [
        "#definição da perda. L = L(critico)+L(ator)\n",
        "huber_loss= tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "def computar_perdas(\n",
        "    action_prob:tf.Tensor,\n",
        "    valores: tf.Tensor,\n",
        "    returns: tf.Tensor\n",
        ")-> tf.Tensor:\n",
        "    vantagem= returns-valores\n",
        "\n",
        "    action_log_prob=tf.math.log(action_prob)\n",
        "    ator_loss= -tf.math.reduce_sum(action_log_prob*vantagem)\n",
        "\n",
        "    critico_loss= huber_loss(valores,returns)\n",
        "\n",
        "    return ator_loss+critico_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0NUrYCs0CPCL"
      },
      "outputs": [],
      "source": [
        "#definindo as etapas de treinamento e o otimizador adam\n",
        "otimizador= tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "@tf.function\n",
        "def train_step(\n",
        "    initial_state: tf.Tensor,\n",
        "    model: tf.keras.Model,\n",
        "    otimizador: tf.keras.optimizers.Optimizer,\n",
        "    gamma:float,\n",
        "    num_max_steps_por_ep:int)-> tf.Tensor:\n",
        "    with tf.GradientTape() as tape:\n",
        "        acoes_prob, valores,recompencas= rodar_ep(initial_state, model,num_max_steps_por_ep)\n",
        "\n",
        "        retornos= receber_valor_esperado(recompencas, gamma)\n",
        "\n",
        "        acoes_prob, valores, retornos=[tf.expand_dims(x,1) for x in [acoes_prob, valores, retornos]]\n",
        "\n",
        "        perda= computar_perdas(acoes_prob, valores, retornos)\n",
        "    grads= tape.gradient(perda, model.trainable_variables)\n",
        "\n",
        "    otimizador.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    recompenca_do_ep=tf.math.reduce_sum(recompencas)\n",
        "\n",
        "    return recompenca_do_ep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNTW_DI-CPCM",
        "outputId": "66a82bfd-773f-4344-be16-11d8f2f79ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|▊         | 794/10000 [02:26<28:23,  5.40it/s, recompanca_do_ep=500, recompenca_rodar=475]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " EP:794 \n",
            " recompanca media 475.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#Loop de treinamento\n",
        "min_ep=100\n",
        "max_ep= 10000\n",
        "max_steps_per_ep=500\n",
        "\n",
        "limiar_de_recompenca=475\n",
        "recompenca_rodar=0\n",
        "\n",
        "gamma=0.99\n",
        "\n",
        "recompancas_do_ep: collections.deque=collections.deque(maxlen=min_ep)\n",
        "\n",
        "t= tqdm.trange(max_ep)\n",
        "for i in t:\n",
        "    estado_inicial,info = env.reset()\n",
        "    estado_inicial= tf.constant(estado_inicial, dtype=tf.float32)\n",
        "    recompanca_do_ep= int(train_step(\n",
        "        estado_inicial,model, otimizador, gamma, max_steps_per_ep)\n",
        "    )\n",
        "\n",
        "    recompancas_do_ep.append(recompanca_do_ep)\n",
        "    recompenca_rodar= statistics.mean(recompancas_do_ep)\n",
        "\n",
        "    t.set_postfix(recompanca_do_ep=recompanca_do_ep, recompenca_rodar=recompenca_rodar)\n",
        "    if recompenca_rodar> limiar_de_recompenca and i >min_ep:\n",
        "        break\n",
        "print(f'\\n EP:{i} \\n recompanca media {recompenca_rodar:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "iSxkIU1WCPCN"
      },
      "outputs": [],
      "source": [
        "#setando a visualização do pendulo invertido\n",
        "render_env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
        "\n",
        "def render_ep(env: gym.Env, model: tf.keras.Model, max_steps:int):\n",
        "    state, info= render_env.reset()\n",
        "    state - tf.constant(state, dtype= tf.float32)\n",
        "    screen= render_env.render()\n",
        "    images= [Image.fromarray(screen)]\n",
        "\n",
        "    for i in range(1, max_steps+1):\n",
        "        state= tf.expand_dims(state, 0)\n",
        "        action_probs, _ = model(state)\n",
        "        action= np.argmax(np.squeeze( action_probs))\n",
        "\n",
        "        state, recompanca, final, truncado, info = render_env.step(action)\n",
        "        state= tf.constant(state,dtype=tf.float32)\n",
        "\n",
        "        if i%10==0:\n",
        "            screen= render_env.render()\n",
        "            images.append(Image.fromarray(screen))\n",
        "\n",
        "        if final:\n",
        "            break\n",
        "    return images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hAziRDyuCPCO"
      },
      "outputs": [],
      "source": [
        "\n",
        "images= render_ep(env, model, max_ep) \n",
        "\n",
        "images[0].save('imagem.gif',save_all=True, append_images=images[1:],loop=0, duration=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kysiR5HGCPCO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "114871b2d14b00e6fd571778f92c60b5f0393057be0a7d7eb7bbe72f43b9e73a"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}